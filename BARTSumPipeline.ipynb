{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5675bc3-1beb-4631-a49b-9c1a5633be33",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fe1be82-0d34-4514-937f-5340e73ceebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/private/b/bergler/h_gurush/tmp/jupyter-envs/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from transformers import BartTokenizer, BartModel, BartForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "178c4db6-5680-4236-bb22-37e353890270",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "169247f7-b808-426d-a8b8-a867f00133dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4ab1f2-953b-4a49-94d9-beeffd10df5b",
   "metadata": {},
   "source": [
    "# Get Dataset from arXiv and PubMed(10 each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd8ff406-c6bc-4b8a-8715-77159911d5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_arXiv_pubMed():\n",
    "    content={}\n",
    "    articles={}\n",
    "    with open(\"Dataset/arxiv-dataset/test.txt\",\"r\") as w:\n",
    "        for index,line in enumerate(w.readlines()):\n",
    "            if index==1000:\n",
    "                break\n",
    "            content=json.loads(line)\n",
    "            articles[content[\"article_id\"]]=content\n",
    "    pubmedArticles={}\n",
    "    with open(\"Dataset/pubmed-dataset/test.txt\",\"r\") as w:\n",
    "        for index,line in enumerate(w.readlines()):\n",
    "            if index==1000:\n",
    "                break\n",
    "            content=json.loads(line)\n",
    "            pubmedArticles[content[\"article_id\"]]=content\n",
    "\n",
    "    return articles, pubmedArticles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8451e7a2-522f-4f90-b633-63432748ef10",
   "metadata": {},
   "source": [
    "### GET MODELS (FINE TUNED and VANILLA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5fa622d-1f3e-412d-9937-8b82ebd47a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartModel, BartForConditionalGeneration\n",
    "\n",
    "def get_model_and_tokenizer(modelName='facebook/bart-large-cnn'):\n",
    "    tokenizer = BartTokenizer.from_pretrained(modelName)\n",
    "    model = BartForConditionalGeneration.from_pretrained(modelName).to(device)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a34f8b84-24a9-462a-bf3c-b58025d63ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fine_tuned_model(modelName=\"BART-E5-ALL.pth\"):\n",
    "    model=BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n",
    "    model.load_state_dict(torch.load(modelName))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65bd3bd1-0a67-4fcf-bb50-62b3443ba6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def section_text_cleaning(sectionText):\n",
    "    section_text=[]\n",
    "    math_mappings={}\n",
    "    for section in sectionText:\n",
    "        cleaned_text=[]\n",
    "        for i in section:\n",
    "            text=i.replace(\"@xcite\",\"\")\n",
    "            for j in text.split():\n",
    "                if \"@xmath\" in j:\n",
    "                    if j in math_mappings:\n",
    "                        text=text.replace(j,math_mappings[j])\n",
    "                    else:\n",
    "                        math_mappings[j]=\"[equation\"+str(len(math_mappings))+\"]\"\n",
    "                        text=text.replace(j,math_mappings[j])\n",
    "            cleaned_text.append(text)\n",
    "        section_text.append(cleaned_text)\n",
    "    return section_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13d8d843-c229-48a7-be8c-0f4b56c5afc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_text_cleaning(docText):\n",
    "    cleaned_text=[]\n",
    "    math_mappings={}\n",
    "    for i in docText:   \n",
    "        text=i.replace(\"@xcite\",\"\")\n",
    "        for j in text.split():\n",
    "            if \"@xmath\" in j:\n",
    "                if j in math_mappings:\n",
    "                    text=text.replace(j,math_mappings[j])\n",
    "                else:\n",
    "                    math_mappings[j]=\"[equation\"+str(len(math_mappings))+\"]\"\n",
    "                    text=text.replace(j,math_mappings[j])\n",
    "        cleaned_text.append(text)\n",
    "        \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7acfeed8-4486-4cd4-b9c7-6b0ff09951c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def handle_size(tokens,default=1000,percent=0.2):\n",
    "    chunked_ip=[]\n",
    "    \n",
    "    #print(tokens.shape[1]/(default-(default*percent)))\n",
    "    for index in range(math.ceil(tokens.shape[1]/(default))):\n",
    "        if index==0:\n",
    "            chunked_ip.append(tokens[ : ,0:default])\n",
    "        else:\n",
    "            st_index=index*math.floor(default-(default*percent))\n",
    "            \n",
    "            \n",
    "            chunked_ip.append(tokens[ : ,st_index: st_index+default])\n",
    "    \n",
    "    return chunked_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39b02f96-60f5-49bc-8537-84e52100db7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_from_sections(section_texts,model,tokenizer):\n",
    "    summary=\"\"\n",
    "    for i in section_texts:\n",
    "        #print(\"before\")\n",
    "        #handle size\n",
    "        \n",
    "        tokens=tokenizer([\" \".join(i)],max_length=1000,padding=\"max_length\",return_tensors=\"pt\")\n",
    "        #print(tokens['input_ids\n",
    "        #print(tokens[\"input_ids\"].shape[1])\n",
    "        if tokens[\"input_ids\"].shape[1]>=1024:\n",
    "            chunked_ip=handle_size(tokens[\"input_ids\"])\n",
    "            for j in chunked_ip:\n",
    "                #print(j.shape)\n",
    "                gen_tokens=model.generate(j.to(device),num_beams=4, min_length=50,max_length=100, early_stopping=True)\n",
    "                decode=tokenizer.batch_decode(gen_tokens,skip_special_tokens=True)\n",
    "                summary+=\" \".join(decode)+\"\\n\"\n",
    "            continue\n",
    "        \n",
    "        #print(\"error\",tokens['input_ids'].shape)\n",
    "        gen_tokens=model.generate(tokens[\"input_ids\"].to(device),num_beams=4, min_length=50,max_length=100, early_stopping=True)\n",
    "        decode=tokenizer.batch_decode(gen_tokens,skip_special_tokens=True)\n",
    "        summary+=\" \".join(decode)+\"\\n\"\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a87193bc-e592-4792-a143-657a583b0607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "def evaluate_summary(summary,reference):\n",
    "    # Initialize the rouge metric\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "    results = rouge.compute(predictions=[summary], references=[reference])\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8daa11-28e5-4cac-973d-73e1e0ba381c",
   "metadata": {},
   "source": [
    "## Centrality Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59351881-9f8a-4436-96d6-1cabeb326d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f26b21b4-e00c-4696-849f-2420213db236",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def create_cosine_matrix(threshold,sent_emb_map):\n",
    "    \n",
    "    #sentence_matrix=[]        \n",
    "    # Calculate pairwise cosine similarity\n",
    "    #for sentence in sentBow[cluster]:\n",
    "    #print(torch.stack(list(cluster_embeddings[cluster].values())).squeeze(1).shape)\n",
    "    similarity_matrix = cosine_similarity(list(sent_emb_map.values()))\n",
    "    for i in range(0,similarity_matrix.shape[0]):\n",
    "        for j in range(0,similarity_matrix.shape[0]):\n",
    "            if similarity_matrix[i,j]>=threshold:\n",
    "                similarity_matrix[i,j]=1\n",
    "            else:\n",
    "                similarity_matrix[i,j]=0\n",
    "        # normalize row by div each element in the row with row sum\n",
    "        similarity_matrix[i]=similarity_matrix[i]/sum(similarity_matrix[i])\n",
    "        \n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e84f1001-2686-46fe-8a95-fd8f5323cb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_transition_matrix(similarity_matrix):\n",
    "    #develop aperodic ,irreducible transition matrix\n",
    "    #damping factor [0.1,0.2]\n",
    "    d=0.1\n",
    "    #uniform probality\n",
    "    uniform_matrix= np.full((1, similarity_matrix.shape[0]), 1 / similarity_matrix.shape[0])\n",
    "    dU=d*uniform_matrix\n",
    "    d_B=(1-d)*similarity_matrix\n",
    "    transition_Matrix=dU+d_B\n",
    "    transition_Matrix=transition_Matrix.T\n",
    "    return transition_Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1231965-2c51-4c9b-a3d8-ea8673a0f2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def compute_centrality(transition_Matrix,threshold):\n",
    "    N=transition_Matrix.shape[0]\n",
    "    p0=np.full(N, 1 / N)\n",
    "    t=0\n",
    "    while True:\n",
    "        t=t+1\n",
    "        #p1=transition_Matrix*p0\n",
    "        p1 = transition_Matrix @ p0\n",
    "        error=np.linalg.norm(p1 - p0)\n",
    "        # print(\"p1\",p1.shape,p0.shape)\n",
    "        # print(\"loop\",t)\n",
    "        if error<threshold:\n",
    "            return p1\n",
    "        p0=p1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39fcf323-d0f0-4c94-a566-a44cb5196cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_lexrank(sentMatrices):\n",
    "   \n",
    "    t_matrix=compute_transition_matrix(sentMatrices)\n",
    "    sent_rank=compute_centrality(t_matrix, 1e-6).tolist()\n",
    "    return sent_rank  \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c5b6416-4f1f-49a9-87ff-bcf43891309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer=BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "# model=BertModel.from_pretrained(\"bert-base-cased\").to(device)\n",
    "sent_model=SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2').to(device)\n",
    "def centroidTfIdf(sentences):\n",
    "    if not all(isinstance(sentence, str) for sentence in sentences):\n",
    "        print(\"All elements in `sentences` must be strings.\",sentences)\n",
    "   \n",
    "        \n",
    "    #sentences=sent_tokenize(\" \".join(clusterContent[clusterContent[\"cluster\"]==cluster][\"content\"]))\n",
    "    embeddings = sent_model.encode(sentences)\n",
    "    #print(type(sentences))\n",
    "    #print(type(embeddings))\n",
    "    sent_list=dict(zip(sentences,embeddings))\n",
    "    #print(\"passed\")\n",
    "  \n",
    "    return sent_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "20c13d2b-acce-451a-8df7-7d29ef087263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocSents_Ranking(text):\n",
    "    #compression rate\n",
    "    r=0.2#0.2\n",
    "    \n",
    "    sent_emb_map=centroidTfIdf(text)\n",
    "    sim_matrix=create_cosine_matrix(0.5,sent_emb_map)# should be 0.5\n",
    "    sent_ranks=compute_lexrank(sim_matrix)\n",
    "    mapped_score=tuple(zip(list(sent_emb_map.keys()),sent_ranks))\n",
    "    \n",
    "    sorted_scores=sorted(mapped_score,key=lambda x:x[1],reverse=True)\n",
    "    n=math.ceil(len(sent_emb_map)*r)\n",
    "    ranked_sents=list(dict(islice(sorted_scores,0,n)).keys())\n",
    "    ordered_sents=[]\n",
    "    for i in sent_emb_map:\n",
    "        if i in ranked_sents:\n",
    "            ordered_sents.append(i)\n",
    "    count_tokens=0\n",
    "    removed_sents=[]\n",
    "    for i in ordered_sents:\n",
    "        if len(i.split())+count_tokens > 1000:\n",
    "            break\n",
    "        else:\n",
    "            removed_sents.append(i)\n",
    "            count_tokens+=len(i.split())\n",
    "    #print(removed_sents)\n",
    "    return removed_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aee7cbf-73ea-485a-811a-99081c0aaf8b",
   "metadata": {},
   "source": [
    "## Put back the @xformula and ref:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bde9f83c-5964-4e89-ab96-f3d06c78b9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_placeholders_text(summary,formula,refs):\n",
    "    refs=dict(zip(list(refs.values()),list(refs.keys())))\n",
    "    formula=dict(zip(list(formula.values()),list(formula.keys())))\n",
    "    \n",
    "       \n",
    "    for i in formula:\n",
    "        summary=summary.replace(i,formula[i])    \n",
    "    return summary\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d7688f-a4d0-47b0-b8c1-b1fad9b5bb8c",
   "metadata": {},
   "source": [
    "## create summary from summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9803ab42-e8c0-44fa-b699-beb6f3983586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_from_summary(section_texts,model,tokenizer):\n",
    "   \n",
    "    summary=\"\"\n",
    "      \n",
    "    #print(\"before\")\n",
    "    #handle size\n",
    "    \n",
    "    tokens=tokenizer(section_texts ,max_length=1000,padding=\"max_length\",return_tensors=\"pt\")\n",
    "    #print(tokens['input_ids\n",
    "    #print(\"here\",tokens[\"input_ids\"][0].shape)\n",
    "    if tokens[\"input_ids\"].shape[1]>=1024:\n",
    "        chunked_ip=handle_size(tokens[\"input_ids\"],1000,0.4)\n",
    "        for j in chunked_ip:\n",
    "            #print(j.shape)\n",
    "            gen_tokens=model.generate(j.to(device),num_beams=4, min_length=50,max_length=100, early_stopping=True)\n",
    "            decode=tokenizer.batch_decode(gen_tokens,skip_special_tokens=True)\n",
    "            summary+=\" \".join(decode)+\"\\n\"\n",
    "        return summary\n",
    "    \n",
    "    #print(\"error\",tokens['input_ids'].shape)\n",
    "    gen_tokens=model.generate(tokens[\"input_ids\"].to(device),num_beams=4, min_length=150,max_length=300, early_stopping=True)\n",
    "    decode=tokenizer.batch_decode(gen_tokens,skip_special_tokens=True)\n",
    "    summary+=\" \".join(decode)+\"\\n\"\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c3f0f61-d65a-4263-81bf-1acda6cbbc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "article=\"\"\"In late September the railroads cut mail service \n",
    "in and out of Moscow, effectively decapitating the postal system and \n",
    "forcing officials to draft a fleet of trucks to move letters in and \n",
    "out of the city. What is clear is that the Post \n",
    "Office and the Railway Ministry both suffer from what ails every Russian \n",
    "venture, private and public alike: Nobody pays his bills. But oddly, real signs of public distress \n",
    "are not particularly common, perhaps because the system rarely seems \n",
    "to shed a part as big as a postal system. ``People are \n",
    "even coming to us, searching for mail that was sent weeks ago.'' Exactly why all this has \n",
    "rumbled to a halt is in some dispute. The government has specified nearly 40 categories \n",
    "of freight which the railroads must carry for next to nothing. A second airport is still \n",
    "demanding 3 million rubles for past-due debts. So much mail is backed \n",
    "up that post offices in Moscow and elsewhere have simply stopped accepting \n",
    "out-of-town mail, except for areas that can be easily reached by truck. And customers are getting angry. Air mail, which \n",
    "amounts to one of every four or five letters, was also suspended at \n",
    "one of Moscow's major airports until this week, when the Post Office \n",
    "coughed up 5 million rubles for old bills.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf78d60f-cd6f-4280-8355-2a66be82694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "def sentSplitter(content):\n",
    "    sentences=sent_tokenize(content)\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7b0ae5-cba3-4e22-9aeb-7405a9c8c5f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "628e7a9b-da9a-4c51-b996-8d89aea95285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "articles=[]\n",
    "for cluster in os.listdir(DATA_LOCATION):\n",
    "    if cluster==\".DS_Store\" or cluster!=\"27\":\n",
    "        continue\n",
    "    \n",
    "    #print(cluster)\n",
    "    for index,document in enumerate(os.listdir(DATA_LOCATION+cluster)):\n",
    "        content=open(DATA_LOCATION+cluster+\"/\"+document,\"r\").read()\n",
    "        sents=sentSplitter(content)\n",
    "        articles.append(sents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a3c549f-eb59-4b71-a8e0-7c4014d9f5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LOCATION=\"Dataset/DUC2004/DUC-2004-Dataset-master/DUC2004_Summarization_Documents/duc2004_testdata/tasks1and2/duc2004_tasks1and2_docs/docs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be88331-7098-4960-8f03-e03a3a3670ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2c6fdab-348f-497d-a1e8-0137b73fb0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "References=(\"Dataset/DUC2004/DUC-2004-Dataset-master/reference/\")\n",
    "summaryReferences={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b34940b1-9d4a-4f1f-a57d-e16c2ad1451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for reference in os.listdir(References):\n",
    "    if \".txt\" not in reference:\n",
    "        print(reference)\n",
    "    else:\n",
    "        task,_=reference.split(\"_\")\n",
    "        task=re.search(r\"[0-9]+\",task)[0]\n",
    "        with open(References+reference,\"r\") as f:\n",
    "            summ=f.read()\n",
    "            if task in summaryReferences:\n",
    "                \n",
    "                summaryReferences[task].append(summ)\n",
    "            else:\n",
    "                summaryReferences[task]=[summ]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8c1903a-c07d-4a8d-956b-3107b7df0b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hari/miniconda3/envs/nlp_bert/lib/python3.10/site-packages/huggingface_hub-0.24.5-py3.8.egg/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'articles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m get_model_and_tokenizer()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#select one article to summarize\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#clean the section text\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m section_text\u001b[38;5;241m=\u001b[39msection_text_cleaning(\u001b[43marticles\u001b[49m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#generate summary\u001b[39;00m\n\u001b[1;32m     12\u001b[0m summary\u001b[38;5;241m=\u001b[39mcreate_summary_from_sections(section_text,model, tokenizer)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'articles' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#initialize model\n",
    "model, tokenizer = get_model_and_tokenizer()\n",
    "\n",
    "\n",
    "#select one article to summarize\n",
    "\n",
    "\n",
    "#clean the section text\n",
    "section_text=section_text_cleaning(articles)\n",
    "\n",
    "#generate summary\n",
    "summary=create_summary_from_sections(section_text,model, tokenizer)\n",
    "\n",
    "\n",
    "print(\"summary\",summary)\n",
    "\n",
    "#evaluate summary\n",
    "print(evaluate_summary(summary,[summaryReferences['27']]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddb4233-dc1b-4fa7-8b3a-667491196fa5",
   "metadata": {},
   "source": [
    "# Running pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cf6b40-e4b9-45cf-98e4-7dc9b2cc7774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#get data\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "arXiv_articles, pubMed_articles=get_data_from_arXiv_pubMed()\n",
    "bertscore = load(\"bertscore\")\n",
    "bleu = evaluate.load(\"google_bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "#initialize model\n",
    "model, tokenizer = get_model_and_tokenizer()\n",
    "\n",
    "fine_model = get_fine_tuned_model()\n",
    "\n",
    "pubMed_scores={\"rouge\":[],\"bleu\":[],\"bertScore\":[]}\n",
    "arXiv_scores={\"rouge\":[],\"bleu\":[],\"bertScore\":[]}\n",
    "section_summary=[]\n",
    "section_fine_summary=[]\n",
    "ranked_summaries=[]\n",
    "ranked_fine_summaries=[]\n",
    "try:\n",
    "    for index,value in enumerate(tqdm(arXiv_articles)):\n",
    "        article=arXiv_articles[value]\n",
    "        section_text=section_text_cleaning(article['sections'])\n",
    "        article_text=doc_text_cleaning(article[\"article_text\"])\n",
    "        doc_sentences=getDocSents_Ranking(article_text)\n",
    "        ranked_summary=create_summary_from_summary(\" \".join(doc_sentences),model, tokenizer)\n",
    "        ranked_fine_summary=create_summary_from_summary(\" \".join(doc_sentences),model, tokenizer)\n",
    "        #generate summary\n",
    "        summary=create_summary_from_sections(section_text,model, tokenizer)\n",
    "        fine_tuned_summary=create_summary_from_sections(section_text,fine_model, tokenizer)\n",
    "        #print(\"summary\",summary)\n",
    "        overall_summary=create_summary_from_summary([summary],model,tokenizer)\n",
    "        overall_fine_summary=create_summary_from_summary([fine_tuned_summary],fine_model,tokenizer)\n",
    "        #evaluate summary\n",
    "        if index%50==0:\n",
    "            section_fine_summary.append(overall_fine_summary)\n",
    "            section_summary.append(overall_summary)\n",
    "            ranked_summaries.append(ranked_summary)\n",
    "            ranked_fine_summaries.append(ranked_fine_summary)\n",
    "        #evaluaate\n",
    "        reference=\" \".join(article['abstract_text']).replace(\"<S>\",\"\").replace(\"</S>\",\"\")\n",
    "        arXiv_scores[\"bertScore\"].append(bertscore.compute(predictions=[overall_summary],references=[reference], lang=\"en\"))\n",
    "        arXiv_scores[\"bertScore\"].append(bertscore.compute(predictions=[overall_fine_summary],references=[reference], lang=\"en\"))\n",
    "        arXiv_scores[\"bertScore\"].append(bertscore.compute(predictions=[ranked_summary],references=[reference], lang=\"en\"))\n",
    "        arXiv_scores[\"bertScore\"].append(bertscore.compute(predictions=[ranked_fine_summary],references=[reference], lang=\"en\"))\n",
    "\n",
    "        #BLEU\n",
    "        arXiv_scores[\"bleu\"].append(bleu.compute(predictions=[overall_summary],references=[reference]))\n",
    "        arXiv_scores[\"bleu\"].append(bleu.compute(predictions=[overall_fine_summary],references=[reference]))\n",
    "        arXiv_scores[\"bleu\"].append(bleu.compute(predictions=[ranked_summary],references=[reference]))\n",
    "        arXiv_scores[\"bleu\"].append(bleu.compute(predictions=[ranked_fine_summary],references=[reference]))\n",
    "\n",
    "        #ROUGE\n",
    "        arXiv_scores[\"rouge\"].append(rouge.compute(predictions=[overall_summary],references=[reference]))\n",
    "        arXiv_scores[\"rouge\"].append(rouge.compute(predictions=[overall_fine_summary],references=[reference]))\n",
    "        arXiv_scores[\"rouge\"].append(rouge.compute(predictions=[ranked_summary],references=[reference]))\n",
    "        arXiv_scores[\"rouge\"].append(rouge.compute(predictions=[ranked_fine_summary],references=[reference]))\n",
    "        if index%100==0:\n",
    "            pickle.dump(arXiv_scores,open(\"arxiv_Scores.pickle\",\"wb\"))\n",
    "            #pickle.dump(pubMed_scores,open(\"pubMed_scores.pickle\",\"wb\"))\n",
    "            torch.cuda.empty_cache()\n",
    "    for index,value in enumerate(tqdm(pubMed_articles)):\n",
    "        article=pubMed_articles[value]\n",
    "        section_text=section_text_cleaning(article['sections'])\n",
    "        article_text=doc_text_cleaning(article[\"article_text\"])\n",
    "        doc_sentences=getDocSents_Ranking(article_text)\n",
    "        ranked_summary=create_summary_from_summary(\" \".join(doc_sentences),model, tokenizer)\n",
    "        ranked_fine_summary=create_summary_from_summary(\" \".join(doc_sentences),model, tokenizer)\n",
    "        #generate summary\n",
    "        summary=create_summary_from_sections(section_text,model, tokenizer)\n",
    "        fine_tuned_summary=create_summary_from_sections(section_text,fine_model, tokenizer)\n",
    "        #print(\"summary\",summary)\n",
    "        overall_summary=create_summary_from_summary([summary],model,tokenizer)\n",
    "        overall_fine_summary=create_summary_from_summary([fine_tuned_summary],fine_model,tokenizer)\n",
    "        #evaluate summary\n",
    "        if index%50==0:\n",
    "            section_fine_summary.append(overall_fine_summary)\n",
    "            section_summary.append(overall_summary)\n",
    "            ranked_summaries.append(ranked_summary)\n",
    "            ranked_fine_summaries.append(ranked_fine_summary)\n",
    "        #evaluaate\n",
    "        reference=\" \".join(article['abstract_text']).replace(\"<S>\",\"\").replace(\"</S>\",\"\")\n",
    "        pubMed_scores[\"bertScore\"].append(bertscore.compute(predictions=[overall_summary],references=[reference], lang=\"en\"))\n",
    "        pubMed_scores[\"bertScore\"].append(bertscore.compute(predictions=[overall_fine_summary],references=[reference], lang=\"en\"))\n",
    "        pubMed_scores[\"bertScore\"].append(bertscore.compute(predictions=[ranked_summary],references=[reference], lang=\"en\"))\n",
    "        pubMed_scores[\"bertScore\"].append(bertscore.compute(predictions=[ranked_fine_summary],references=[reference], lang=\"en\"))\n",
    "\n",
    "        #BLEU\n",
    "        pubMed_scores[\"bleu\"].append(bleu.compute(predictions=[overall_summary],references=[reference]))\n",
    "        pubMed_scores[\"bleu\"].append(bleu.compute(predictions=[overall_fine_summary],references=[reference]))\n",
    "        pubMed_scores[\"bleu\"].append(bleu.compute(predictions=[ranked_summary],references=[reference]))\n",
    "        pubMed_scores[\"bleu\"].append(bleu.compute(predictions=[ranked_fine_summary],references=[reference]))\n",
    "\n",
    "        #ROUGE\n",
    "        pubMed_scores[\"rouge\"].append(rouge.compute(predictions=[overall_summary],references=[reference]))\n",
    "        pubMed_scores[\"rouge\"].append(rouge.compute(predictions=[overall_fine_summary],references=[reference]))\n",
    "        pubMed_scores[\"rouge\"].append(rouge.compute(predictions=[ranked_summary],references=[reference]))\n",
    "        pubMed_scores[\"rouge\"].append(rouge.compute(predictions=[ranked_fine_summary],references=[reference]))\n",
    "        if index%100==0:\n",
    "            #pickle.dump(arXiv_scores,open(\"arxiv_Scores.pickle\",\"wb\"))\n",
    "            pickle.dump(pubMed_scores,open(\"pubMed_scores.pickle\",\"wb\"))\n",
    "            torch.cuda.empty_cache()\n",
    "except Exception as e:\n",
    "    print(\"error\",e)\n",
    "    \n",
    "finally:\n",
    "    pickle.dump(arXiv_scores,open(\"arxiv_Scores.pickle\",\"wb\"))\n",
    "    pickle.dump(pubMed_scores,open(\"pubMed_scores.pickle\",\"wb\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e4bd80d-b446-431c-ae53-8e2b3941b70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(section_fine_summary,open(\"fine summary.pickle\",\"wb\"))\n",
    "#pickle.dump(ranked_summaries,open(\"ranked summary.pickle\",\"wb\"))\n",
    "#pickle.dump(section_summary,open(\"summary.pickle\",\"wb\"))\n",
    "pickle.dump(ranked_fine_summaries,open(\"ranked fine summary.pickle\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfc8bd3c-bd4c-4e74-a8df-80402124c3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk/nobackup/ipykernel_3288533/411595313.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(modelName))\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 1000/1000 [24:36<00:00,  1.48s/it]\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "#get data\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "arXiv_articles, pubMed_articles=get_data_from_arXiv_pubMed()\n",
    "bertscore = load(\"bertscore\")\n",
    "bleu = evaluate.load(\"google_bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "#initialize model\n",
    "model, tokenizer = get_model_and_tokenizer()\n",
    "\n",
    "fine_model = get_fine_tuned_model()\n",
    "\n",
    "pubMed_scores={\"rouge\":[],\"bleu\":[],\"bertScore\":[]}\n",
    "arXiv_scores={\"rouge\":[],\"bleu\":[],\"bertScore\":[]}\n",
    "section_summary=[]\n",
    "section_fine_summary=[]\n",
    "ranked_summaries=[]\n",
    "ranked_fine_summaries=[]\n",
    "try:\n",
    "    for index,value in enumerate(tqdm(arXiv_articles)):\n",
    "        if index<900:\n",
    "            continue\n",
    "        article=arXiv_articles[value]\n",
    "        #print(value)\n",
    "        section_text=section_text_cleaning(article['sections'])\n",
    "        article_text=doc_text_cleaning(article[\"article_text\"])\n",
    "        doc_sentences=getDocSents_Ranking(article_text)\n",
    "        #ranked_summary=create_summary_from_summary(\" \".join(doc_sentences),model, tokenizer)\n",
    "        ranked_fine_summary=create_summary_from_summary(\" \".join(doc_sentences),fine_model, tokenizer)\n",
    "        #generate summary\n",
    "        #summary=create_summary_from_sections(section_text,model, tokenizer)\n",
    "        fine_tuned_summary=create_summary_from_sections(section_text,fine_model, tokenizer)\n",
    "        #print(\"summary\",summary)\n",
    "        #overall_summary=create_summary_from_summary([summary],model,tokenizer)\n",
    "        overall_fine_summary=create_summary_from_summary([fine_tuned_summary],fine_model,tokenizer)\n",
    "        #print(\"summary\",overall_fine_summary)\n",
    "        #evaluate summary\n",
    "        if index%50==0:\n",
    "            section_fine_summary.append(overall_fine_summary)\n",
    "            ranked_fine_summaries.append(ranked_fine_summary)\n",
    "        #evaluaate\n",
    "        reference=\" \".join(article['abstract_text']).replace(\"<S>\",\"\").replace(\"</S>\",\"\")\n",
    "       \n",
    "        arXiv_scores[\"bertScore\"].append(bertscore.compute(predictions=[overall_fine_summary],references=[reference], lang=\"en\"))\n",
    "        #arXiv_scores[\"bertScore\"].append(bertscore.compute(predictions=[ranked_summary],references=[reference], lang=\"en\"))\n",
    "        arXiv_scores[\"bertScore\"].append(bertscore.compute(predictions=[ranked_fine_summary],references=[reference], lang=\"en\"))\n",
    "\n",
    "        #BLEU\n",
    "        #arXiv_scores[\"bleu\"].append(bleu.compute(predictions=[overall_summary],references=[reference]))\n",
    "        arXiv_scores[\"bleu\"].append(bleu.compute(predictions=[overall_fine_summary],references=[reference]))\n",
    "        #arXiv_scores[\"bleu\"].append(bleu.compute(predictions=[ranked_summary],references=[reference]))\n",
    "        arXiv_scores[\"bleu\"].append(bleu.compute(predictions=[ranked_fine_summary],references=[reference]))\n",
    "\n",
    "        #ROUGE\n",
    "        #arXiv_scores[\"rouge\"].append(rouge.compute(predictions=[overall_summary],references=[reference]))\n",
    "        arXiv_scores[\"rouge\"].append(rouge.compute(predictions=[overall_fine_summary],references=[reference]))\n",
    "        #arXiv_scores[\"rouge\"].append(rouge.compute(predictions=[ranked_summary],references=[reference]))\n",
    "        arXiv_scores[\"rouge\"].append(rouge.compute(predictions=[ranked_fine_summary],references=[reference]))\n",
    "        if index%100==0:\n",
    "            pickle.dump(arXiv_scores,open(\"arxiv_Scores_ranked_fine_epoch_5_901_1000.pickle\",\"wb\"))\n",
    "            #pickle.dump(pubMed_scores,open(\"pubMed_scores_ranked_fine.pickle\",\"wb\"))\n",
    "            torch.cuda.empty_cache()\n",
    "    for index,value in enumerate(tqdm(pubMed_articles)):\n",
    "        break\n",
    "        article=pubMed_articles[value]\n",
    "        section_text=section_text_cleaning(article['sections'])\n",
    "        article_text=doc_text_cleaning(article[\"article_text\"])\n",
    "        doc_sentences=getDocSents_Ranking(article_text)\n",
    "        #ranked_summary=create_summary_from_summary(\" \".join(doc_sentences),model, tokenizer)\n",
    "        ranked_fine_summary=create_summary_from_summary(\" \".join(doc_sentences),fine_model, tokenizer)\n",
    "        #generate summary\n",
    "        #summary=create_summary_from_sections(section_text,model, tokenizer)\n",
    "        fine_tuned_summary=create_summary_from_sections(section_text,fine_model, tokenizer)\n",
    "        #print(\"summary\",summary)\n",
    "        #overall_summary=create_summary_from_summary([summary],model,tokenizer)\n",
    "        overall_fine_summary=create_summary_from_summary([fine_tuned_summary],fine_model,tokenizer)\n",
    "        #evaluate summary\n",
    "        if index%50==0:\n",
    "           \n",
    "            section_fine_summary.append(overall_fine_summary)\n",
    "            ranked_fine_summaries.append(ranked_fine_summary)\n",
    "        #evaluaate\n",
    "        reference=\" \".join(article['abstract_text']).replace(\"<S>\",\"\").replace(\"</S>\",\"\")\n",
    "        \n",
    "        #pubMed_scores[\"bertScore\"].append(bertscore.compute(predictions=[overall_summary],references=[reference], lang=\"en\"))\n",
    "        \n",
    "        #pubMed_scores[\"bertScore\"].append(bertscore.compute(predictions=[ranked_summary],references=[reference], lang=\"en\"))\n",
    "        pubMed_scores[\"bertScore\"].append(bertscore.compute(predictions=[ranked_fine_summary],references=[reference], lang=\"en\"))\n",
    "        pubMed_scores[\"bertScore\"].append(bertscore.compute(predictions=[overall_fine_summary],references=[reference], lang=\"en\"))\n",
    "\n",
    "        #BLEU\n",
    "        #pubMed_scores[\"bleu\"].append(bleu.compute(predictions=[overall_summary],references=[reference]))\n",
    "        \n",
    "        #pubMed_scores[\"bleu\"].append(bleu.compute(predictions=[ranked_summary],references=[reference]))\n",
    "        pubMed_scores[\"bleu\"].append(bleu.compute(predictions=[ranked_fine_summary],references=[reference]))\n",
    "        pubMed_scores[\"bleu\"].append(bleu.compute(predictions=[overall_fine_summary],references=[reference]))\n",
    "\n",
    "        #ROUGE\n",
    "        #pubMed_scores[\"rouge\"].append(rouge.compute(predictions=[overall_summary],references=[reference]))\n",
    "       \n",
    "        #pubMed_scores[\"rouge\"].append(rouge.compute(predictions=[ranked_summary],references=[reference]))\n",
    "        pubMed_scores[\"rouge\"].append(rouge.compute(predictions=[ranked_fine_summary],references=[reference]))\n",
    "        pubMed_scores[\"rouge\"].append(rouge.compute(predictions=[overall_fine_summary],references=[reference]))\n",
    "        if index%100==0:\n",
    "            #pickle.dump(arXiv_scores,open(\"arxiv_Scores.pickle\",\"wb\"))\n",
    "            pickle.dump(pubMed_scores,open(\"pubMed_scores_ranked_fine_epoch_5_201_204.pickle\",\"wb\"))\n",
    "            torch.cuda.empty_cache()\n",
    "except Exception as e:\n",
    "    print(\"error\",e)\n",
    "    \n",
    "finally:\n",
    "    pickle.dump(arXiv_scores,open(\"arxiv_Scores_ranked_fine_epoch_5_901-1000.pickle\",\"wb\"))\n",
    "    #pickle.dump(pubMed_scores,open(\"pubMed_scores_ranked_epoch_5_201_204_complete.pickle\",\"wb\"))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
