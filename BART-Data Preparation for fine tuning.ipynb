{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7285e8ab-455a-4b4f-8317-cee20065e926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/private/b/bergler/h_gurush/tmp/jupyter-envs/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "from transformers import BartTokenizer, BartModel, BartForConditionalGeneration\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20b0654b-30b3-4c5b-8cd8-333e5a56c801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")#CAUTION: RUN THIS C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78196c08-7321-405d-899a-b8d52285fe3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43b7f218-5722-454a-9f78-38025628cc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c799666-e50e-444a-b669-83731bf9eb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "331fb2d1-2797-48dc-9c26-a15aa01679b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def get_data_from_arXiv_pubMed():\n",
    "    content={}\n",
    "    articles={}\n",
    "    \n",
    "    articles_docs_abstract = {}\n",
    "    with open(\"Dataset/arxiv-dataset/train.txt\",\"r\") as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                article = json.loads(line.strip())\n",
    "                articles_docs_abstract[\" \".join(article['article_text'])] = \" \".join(article.get('abstract_text'))\n",
    "               \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping malformed line: {e}\")\n",
    "    pubmedArticles_docs_abs={}\n",
    "    with open(\"Dataset/pubmed-dataset/train.txt\",\"r\") as w:\n",
    "        for index,line in enumerate(w.readlines()):\n",
    "            \n",
    "               \n",
    "            content=json.loads(line)\n",
    "            pubmedArticles_docs_abs[\" \".join(content['article_text'])] =\" \".join(list(content.get('abstract_text')))\n",
    "          \n",
    "                   \n",
    "                \n",
    "    return pubmedArticles_docs_abs,articles_docs_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b610f20c-e40a-430a-9605-8d1c1444fa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "pubMed,articles_dict=get_data_from_arXiv_pubMed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13183105-272b-424e-b171-3d57add3a489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5603db-b657-4ec5-b205-0ed7aca8ad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# articles_dict=pickle.load(open(\"article_dict.pickle\",\"rb\"))\n",
    "#pubMed=pickle.load(open(\"pubmed.pickle\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83fb76b2-6068-4140-96b7-6051aefa6988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(articles_dict,open(\"article_dict.pickle\",\"wb\"))\n",
    "# pickle.dump(pubMed,open(\"pubmed.pickle\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f44e283e-9703-4d12-a995-32f3cac9adc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartModel, BartForConditionalGeneration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59d02959-feda-4e1a-bdfa-ac8d73f4e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5bed13-6dd0-4b49-938f-cf2b82602779",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use config to see the details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bdae547-bc78-4ce1-b070-12061dfe0411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a76c39e3-00a3-4fd2-9d58-46926e12dd67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5202e5454584f2380ce2440bbc4867f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd0b1aafff84c018bd699a13f020bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122326fa119945df925d3ddd6e45ec03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\").to\n",
    "\n",
    "# Tokenize and preprocess the data\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=5000)\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "encoded_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91092208-38cd-445d-a808-9d23540bc52f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(encoded_dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c04523-a71e-42d7-bbca-de37be8bb073",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd0011e-d9a1-4525-b570-73e593380439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def cleaned_sent(doc):\n",
    "    for index in range(0,len(doc)):\n",
    "        sent=re.sub(r\"@xcite[0-9]+\",\" \",doc[index]) #remove citation placeholder\n",
    "        sent=re.sub(r\"@xmath([0-9]+)\",r\"formula\\1\",sent) #replace formula placholder\n",
    "        doc[index]=sent\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e74cfc3b-dc45-4006-ab48-20b9c7692fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=None\n",
    "import pickle\n",
    "def preprocess_documents_with_striding(documents,tokenizer,model,striding_rate=0.2,max_tokens=1020):\n",
    "    input_ids_list =[]\n",
    "    striding_count=math.floor(max_tokens-(max_tokens*striding_rate))\n",
    "    reduced_document=[]\n",
    "    attention_mask=[]\n",
    "    cleaned_docs=cleaned_sent(documents)\n",
    "    tokens=[]\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        for doc in tqdm(cleaned_docs):\n",
    "           \n",
    "            # remove citations and replace equation placeholder with better placeholders\n",
    "            try:\n",
    "                \n",
    "                tokens=tokenizer(doc,truncation=False,padding=True,return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "                \n",
    "                loop_count=math.ceil(tokens.shape[1]/striding_count)\n",
    "                if loop_count==0:\n",
    "                    continue\n",
    "                target_length_max=math.ceil(max_tokens/loop_count)\n",
    "                #80%max is min\n",
    "                \n",
    "                target_length_min=math.ceil(target_length_max-(target_length_max*0.2))\n",
    "                doc_summary=[]\n",
    "                #tokens=tokenizer(\" \".join(cleaned_doc),max_length=math.ceil(loop_count*max_tokens),padding=\"max_length\",return_tensors=\"pt\").to(device)\n",
    "                #print(tokens[\"input_ids\"].shape,loop_count)\n",
    "                for i in range(0,loop_count):\n",
    "                    \n",
    "                    st_index=i*striding_count\n",
    "                    end_index=st_index+max_tokens\n",
    "                    #sequence=\" \".join(cleaned_doc[st_index:end_index])\n",
    "                    \n",
    "                    #print(\"sent\",len(cleaned_doc[st_index:end_index]))\n",
    "                    #tokens=tokenizer(sequence,max_length=max_tokens,padding=\"max_length\",return_tensors=\"pt\",truncation=True)\n",
    "                    #print(tokens[\"input_ids\"][0,st_index:end_index].unsqueeze(0).shape)\n",
    "                    gen_tokens=model.generate(tokens[0,st_index:end_index].unsqueeze(0),num_beams=4, max_length=target_length_max,min_length=target_length_min, early_stopping=True)\n",
    "                    #print(gen_tokens)\n",
    "                    #print(gen_tokens.shape)\n",
    "                    #decode=tokenizer.decode(gen_tokens.squeeze(0),skip_special_tokens=True)\n",
    "                    doc_summary.append(gen_tokens.squeeze(0).cpu())\n",
    "                docss=torch.cat(doc_summary)\n",
    "                #print(\"shape of op\",docss.shape)\n",
    "                if docss.shape[0]>1024:\n",
    "                    \n",
    "                    docss=docss[0:1024]\n",
    "                if docss.shape[0]<1024:\n",
    "                    #print(\"no error here 0\")\n",
    "                    docss = torch.cat((docss,torch.tensor([tokenizer.pad_token_id or 0] * int(1024 - docss.shape[0]))))\n",
    "                    #print(\"no error here\")\n",
    "                #print(docss.shape)\n",
    "                output_attention_mask = (docss != tokenizer.pad_token_id).long()\n",
    "                reduced_document.append(docss)\n",
    "                attention_mask.append(output_attention_mask)\n",
    "                torch.cuda.empty_cache()\n",
    "            except Exception as e:\n",
    "                print(\"error\",e)\n",
    "                continue\n",
    "    return torch.stack(reduced_document).to(device),torch.stack(attention_mask).to(device)\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b37c3cb-64be-4edb-a474-3e0624ce1596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_abstract(abstracts):\n",
    "    cleaned_abstracts=[]\n",
    "    for i in abstracts:\n",
    "        text=re.sub(r\"@xcite[0-9]+\", \"\",i)\n",
    "        text=re.sub(r\"@xmath([0-9]+)\",r\"equation\\1\",text)\n",
    "        cleaned_abstracts.append(text)\n",
    "    return cleaned_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "121e51c2-661a-4476-914c-3b47f36eb1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer\n",
    "from tqdm import tqdm\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "striding_model=BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\").to(device)\n",
    "def preprocess_data(articles_dict,max_input_length=1024):\n",
    "    \"\"\"\n",
    "    Tokenize the input (scientific document) and target (abstract).\n",
    "    \"\"\"\n",
    "    input_ids=[]\n",
    "    attention_mask=[]\n",
    "    target_ids=[]\n",
    "    count=0\n",
    "    #for i in tqdm(articles_dict):\n",
    "        # if count!=2:\n",
    "        #     count+=1\n",
    "        #     continue\n",
    "   \n",
    "    document,atn_mask=preprocess_documents_with_striding(list(articles_dict.keys())[0:10000],tokenizer,striding_model)\n",
    "    abstract=clean_abstract(list(articles_dict.values())[0:10000])\n",
    "    print(\"abstract\",len(abstract))\n",
    "\n",
    "\n",
    "    #documents=preprocess_documents_with_striding(documents,tokenizer,striding_model)\n",
    "    #abstracts=clean_abstract(abstracts)\n",
    "    \n",
    "    # inputs = tokenizer.batch_decode(\n",
    "    #     document, \n",
    "    #     max_length=max_input_length, \n",
    "    #     truncation=True, \n",
    "    #     padding=\"max_length\", \n",
    "    #     return_tensors=\"pt\"\n",
    "    # )\n",
    "    inputs={\"input_ids\":document,\"attention_mask\":atn_mask}\n",
    "    \n",
    "    targets = tokenizer(\n",
    "        abstract, \n",
    "        max_length=max_input_length, \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    #targets=None\n",
    "    # input_ids.append(inputs[\"input_ids\"][0])\n",
    "    # attention_mask.append(inputs[\"attention_mask\"][0])\n",
    "    # target_ids.append(targets[\"input_ids\"][0])\n",
    "    # inputs={\"input_ids\":torch.stack(input_ids,dim=0).to(device),\"attention_mask\":torch.stack(attention_mask,dim=0).to(device)}\n",
    "    # targets={\"input_ids\":torch.stack(target_ids,dim=0).to(device)}\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f8ebbfa-6edb-453a-9982-470ce8497b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.inputs[\"input_ids\"][idx],\n",
    "            'attention_mask': self.inputs[\"attention_mask\"][idx],\n",
    "            'labels': self.targets[\"input_ids\"][idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "e03e1bb6-f909-4c71-8579-36054ef91fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 7924/10000 [23:38:24<5:48:44, 10.08s/it] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 10000/10000 [29:57:02<00:00, 10.78s/it] \n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "# inputs=[]\n",
    "# targets=[]\n",
    "# with multiprocessing.Pool(processes=2) as pool:\n",
    "#     # Apply tqdm for tracking\n",
    "#     for inp,target in tqdm(pool.imap(preprocess_data, articles_dict), total=len(articles_dict)):\n",
    "#         inputs.extend(inp)\n",
    "#         targets.extend(target)\n",
    "torch.cuda.empty_cache()\n",
    "inputs, targets = preprocess_data(articles_dict)\n",
    "pickle.dump(inputs,open(\"articles_inputs.pickle\",\"wb\"))\n",
    "pickle.dump(targets,open(\"articles_targets.pickle\",\"wb\"))\n",
    "inputs1, targets1 = preprocess_data(pubMed)\n",
    "pickle.dump(inputs1,open(\"pubMed_inputs.pickle\",\"wb\"))\n",
    "pickle.dump(targets1,open(\"pubMed_targets.pickle\",\"wb\"))\n",
    "dataset = SummarizationDataset(inputs, targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
